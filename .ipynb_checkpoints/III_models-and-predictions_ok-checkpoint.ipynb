{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7161f50f-14de-4a00-9951-535df873c0d2",
   "metadata": {},
   "source": [
    "# Project 2: Covid ---> III/ Models and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea38c3b3-4b5c-4346-a021-66fd167ed5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9743d5-e58e-4dd8-b8b2-7a4eed760c9b",
   "metadata": {},
   "source": [
    "## 1. Literature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae98ebb-8302-4241-b8f5-8c14173f3453",
   "metadata": {},
   "source": [
    "In 2019, the first COVID-19 cases are observed in China. Rapidly, the SARS-Cov2 virus spread worldwide, pushing governments to take strict decisions about the lives of their co-citizens, like containment, to protect the population. Indeed, in some cases, COVID-19 patients ended up in intensive care services and sometimes died.\n",
    "\n",
    "The aim of our model is, based on easily computable parameters at the study's beginning, to predict whether the patient will be likely to die or if the chance of survival is important. The point of this study is to help the hospital organise in the case of a high number of cases.\n",
    "\n",
    "\n",
    "The studied dataset stem from the IDDO Data Repository of COVID-19 data. This data was pulled from the underlying data collection projects on 2022-09-01. The data comes from 1,200 institutions from over 45 countries and gather various information from 700,000 hospitalised individuals.\n",
    "\n",
    "To keep only the relevant features, we first dive into the literature, using Meta-analysis papers. First, we have been looking for aggravating factors that will likely lead the patient to ICU.\n",
    "\n",
    "Obesity: according to a meta-analysis by Sales-Peres, there is a correlation between obesity and ICU admission. This paper also concluded that co-morbidities for obese patients, such as hypertension, type 2 diabetes, smoking habit, lung disease, and/or cardiovascular disease lead to a higher chance of ICU admission.\n",
    "Age: patients aged 70 years and above have a higher risk of infection and a higher need for intensive care than patients younger than 70.\n",
    "Sex: men, when infected, have a higher risk of severe COVID-19 disease and a higher need for intensive care than women\\cite{pijls_demographic_2021}.\n",
    "Ethnicity: the risk of contamination was higher in most ethnic minority groups than their White counterparts in North America and Europe. Among people with confirmed infection, African-Americans and Hispanic Americans were also more likely than White Americans to be hospitalised with SARS-CoV-2 infection. However, the probability of ICU admission was equivalent for all groups. Thus, ethnicity is not relevant to our question. \n",
    "Blood tests: Patients with increased pancreatic enzymes, including elevated serum lipase or amylase of either type, had worse clinical outcomes. Lower levels of lymphocytes and hemoglobin; elevated levels of leukocytes, aspartate aminotransferase, alanine aminotransferase, blood creatinine, blood urea nitrogen, high-sensitivity troponin, creatine kinase, high-sensitivity C-reactive protein, interleukin 6, D-dimer, ferritin, lactate dehydrogenase, and procalcitonin; and a high erythrocyte sedimentation rate were also associated with severe COVID-19.  \n",
    "\n",
    "Out of a total of 3009 citations, 17 articles (22 studies, 21 from China and one study from Singapore) with 3396 ranging from 12 to1099 patients were included. Our meta-analyses showed a significant decrease in lymphocyte, monocyte, and eosinophil, hemoglobin, platelet, albumin, serum sodium, lymphocyte to C-reactive protein ratio (LCR), leukocyte to C-reactive protein ratio (LeCR), leukocyte to IL-6 ratio (LeIR), and an increase in the neutrophil, alanine aminotransferase (ALT), aspartate aminotransferase (AST), total bilirubin, blood urea nitrogen (BUN), creatinine (Cr), erythrocyte Sedimentation Rate (ESR), C-reactive protein (CRP), Procalcitonin (PCT), lactate dehydrogenase (LDH), fibrinogen, prothrombin time (PT), D-dimer, glucose level, and neutrophil to lymphocyte ratio (NLR) in the severe group compared with the non-severe group. \n",
    "\n",
    "No significant changes in white blood cells (WBC), Creatine Kinase (CK), troponin I, myoglobin, IL-6 and K between the two groups were observed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9250aa15-370f-4871-af7b-abe871e57467",
   "metadata": {},
   "source": [
    "## 2. Load data after data_selection and feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b89e6596-eb3c-4383-9d82-b4cf9f55e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file\n",
    "data_folder = op.join(os.getcwd(), \"data\", \"results\")\n",
    "mylist = []\n",
    "for chunk in pd.read_csv(op.join(data_folder, 'df_final_I-DataSelection.csv'), sep=',', low_memory=False, chunksize=5000, index_col = 0):\n",
    "    mylist.append(chunk)\n",
    "df = pd.concat(mylist, axis=0)\n",
    "df.name = 'df'\n",
    "del mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fed57e35-089f-4979-bbaa-e4e657a24d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete row where DSDECOD is NA\n",
    "df = df[df.DSDECOD.notna()]\n",
    "df.DSDECOD.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b730981c",
   "metadata": {},
   "source": [
    "## 3. Search the best model/algorithm and the best parameters\n",
    "\n",
    "We will work only on a sample of the data to try to find the best algorithm/model with the best parameters. Then we will apply the best model to the whole data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b5b93837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a sample of the data\n",
    "df_sample = df.sample(n = 10000, axis = 0, random_state = 42, replace = False)\n",
    "\n",
    "# Separate into features and label\n",
    "X = df_sample.loc[:, df_sample.columns != 'DSDECOD'].to_numpy()\n",
    "y = df_sample.loc[:, df_sample.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c68612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For storage of results for each model\n",
    "df_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ade6658-bec2-4d54-8557-00d697033f7f",
   "metadata": {},
   "source": [
    "### 3.1 Model 1: Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7e3b2b-fa29-4efc-acbc-e651ba631be6",
   "metadata": {},
   "source": [
    "#### 3.1.1 Basic method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6599f925-4e1d-4dad-8129-bfd8ac822b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for logistic regression :  0.7735652173913043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_reg = Pipeline([('clf', LogisticRegression())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_reg.fit(X_train, y_reg_train)\n",
    "\n",
    "# Predicting values\n",
    "y_reg_pred = pipe_reg.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_reg_pred, y_reg_test)\n",
    "print('Accuracy score for logistic regression : ',acc_score)\n",
    "\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"logistic regression\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccb8beed-ff36-4bf7-8e12-ae1c87cd7788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03163098, -0.03163098,  0.07162374,  0.98571667,  0.09700267,\n",
       "        -0.40978083,  0.0519647 ,  0.41043135, -0.03441706,  0.01274007,\n",
       "         0.11726346,  0.02119725, -0.39100685, -0.18191565, -0.07475868,\n",
       "         0.08623388,  0.02837382,  0.15915803,  0.06301954,  0.06905836,\n",
       "         0.0503761 ,  0.06332276,  0.10889381,  0.06332276,  0.22128865,\n",
       "        -0.11430464,  0.03990982,  0.02850262,  0.00954175, -0.10633301,\n",
       "        -0.09238852,  0.06332276,  0.22128865,  0.06857301, -0.38641174,\n",
       "         0.        , -0.08333206,  0.00973305,  0.        , -0.03639733,\n",
       "         0.1181282 , -0.01864331,  0.        ,  0.26474536,  0.06689652,\n",
       "         0.0503761 , -0.31220915, -0.06315157,  0.        ,  0.0248374 ,\n",
       "         0.06531094,  0.03766175,  0.01583329,  0.00712008,  0.02470179,\n",
       "        -0.03246636, -0.02702797,  0.03713892,  0.06925871, -0.02318937,\n",
       "         0.02580156, -0.00823818,  0.02079294,  0.02535451,  0.01409555,\n",
       "        -0.06076724, -0.02655724,  0.0874436 , -0.05867805,  0.03354859,\n",
       "        -0.06371303,  0.0119314 , -0.00970574, -0.05944048, -0.02176724,\n",
       "         0.05020536,  0.        , -0.19334004,  0.22053049, -0.04388425,\n",
       "         0.10117214,  0.21792136,  0.04960914,  0.        , -0.01917924,\n",
       "         0.06684545,  0.0142472 , -0.04539385,  0.10649305, -0.0329322 ,\n",
       "        -0.05226139]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See coeffs of the model\n",
    "pipe_reg.named_steps['clf'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cd4544-deca-40fa-aa3a-85864d34353d",
   "metadata": {},
   "source": [
    "### 3.1.2 Play with logistic regression parameters, cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c0e6d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['C', 'class_weight', 'dual', 'fit_intercept', 'intercept_scaling', 'l1_ratio', 'max_iter', 'multi_class', 'n_jobs', 'penalty', 'random_state', 'solver', 'tol', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa60bd85-2372-4183-aa3e-d77f917307ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 4 is smaller than n_iter=30. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sande\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.78753627        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\sande\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for logistic regression :  0.7735652173913043\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20784/1980668042.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy score for logistic regression : '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0macc_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "# Perform Logistic regression with cross-validation\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_reg = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_reg = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_reg_woNaN = imputer.fit_transform(X_reg)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_reg_woNaN = (X_reg_woNaN[~np.isnan(y_reg)[:,0], :])\n",
    "y_reg_woNaN = y_reg[~np.isnan(y_reg)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg_woNaN, y_reg_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_reg = Pipeline([('scl', StandardScaler()), ('clf', LogisticRegression())])\n",
    "\n",
    "# Set parameters to test\n",
    "param_reg = {'clf__penalty': [None, 'l1', 'l2', 'elasticnet']}\n",
    "\n",
    "# Cross-validation\n",
    "cv_reg = RandomizedSearchCV(estimator = pipe_reg, \n",
    "                                         param_distributions=param_reg, \n",
    "                                         cv=3, n_iter=30, n_jobs=-1)\n",
    "\n",
    "# Fit data into the model\n",
    "cv_reg.fit(X_reg_train, y_reg_train)\n",
    "\n",
    "# Predicting values\n",
    "y_reg_pred = cv_reg.predict(X_reg_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_reg_pred, y_reg_test)\n",
    "print('Accuracy score for logistic regression : ',acc_score)\n",
    "\n",
    "clf.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db92e6b1-ec63-4085-b34d-d901e251838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See best parameters of the model\n",
    "g_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af7203b-6f23-4968-8990-d8dc685b6f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See coeffs of the model\n",
    "cv_reg.named_steps['clf'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f809a66e-2d05-4fa0-a610-8bb9f430c68f",
   "metadata": {},
   "source": [
    "## 3.2 Model 2: GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6116709-d110-41d1-b5c5-2c118b69d7bd",
   "metadata": {},
   "source": [
    "Estimators that allow NaN values for type classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c8dc25-a3e1-42c0-96e0-9324393eeae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform GradientBoostingClassifier\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_hgbc = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_hgbc = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_hgbc_woNaN = imputer.fit_transform(X_hgbc)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_hgbc_woNaN = (X_hgbc_woNaN[~np.isnan(y_hgbc)[:,0], :])\n",
    "y_hgbc_woNaN = y_hgbc[~np.isnan(y_hgbc)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_hgbc_train, X_hgbc_test, y_hgbc_train, y_hgbc_test = train_test_split(X_hgbc_woNaN, y_hgbc_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_hgbc = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', GradientBoostingClassifier())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_hgbc.fit(X_hgbc_train, y_hgbc_train)\n",
    "\n",
    "# Predicting values\n",
    "y_hgbc_pred = pipe_hgbc.predict(X_hgbc_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_hgbc_pred, y_hgbc_test)\n",
    "print('Accuracy score for gradient boosting classification : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"gradient boosting classifier\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13710c8b",
   "metadata": {},
   "source": [
    "## 3.3 Model 3 : K-Nearest Neighbor Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dabc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform KNeighborsClassifier\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_knn = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_knn = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_knn_woNaN = imputer.fit_transform(X_knn)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_knn_woNaN = (X_knn_woNaN[~np.isnan(y_knn)[:,0], :])\n",
    "y_knn_woNaN = y_knn[~np.isnan(y_knn)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_knn_train, X_knn_test, y_knn_train, y_knn_test = train_test_split(X_knn_woNaN, y_knn_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_knn = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', KNeighborsClassifier())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_knn.fit(X_knn_train, y_knn_train)\n",
    "\n",
    "# Predicting values\n",
    "y_knn_pred = pipe_knn.predict(X_knn_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_knn_pred, y_knn_test)\n",
    "print('Accuracy score for k-nearest neighbor classification : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"k-nearest neighbors\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6820d6b8",
   "metadata": {},
   "source": [
    "## 3.4 Model 4 : Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c28b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform GaussianNB\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_gnb = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_gnb = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_gnb_woNaN = imputer.fit_transform(X_gnb)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_gnb_woNaN = (X_gnb_woNaN[~np.isnan(y_gnb)[:,0], :])\n",
    "y_gnb_woNaN = y_gnb[~np.isnan(y_gnb)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_gnb_train, X_gnb_test, y_gnb_train, y_gnb_test = train_test_split(X_gnb_woNaN, y_gnb_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_gnb = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', GaussianNB())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_gnb.fit(X_gnb_train, y_gnb_train)\n",
    "\n",
    "# Predicting values\n",
    "y_gnb_pred = pipe_gnb.predict(X_gnb_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_gnb_pred, y_gnb_test)\n",
    "print('Accuracy score for naive Bayes : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"naive Bayes\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b1a9e",
   "metadata": {},
   "source": [
    "## 3.5 Model 5 : Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e7f3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform RandomForestClassifier\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_rfc = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_rfc = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_rfc_woNaN = imputer.fit_transform(X_rfc)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_rfc_woNaN = (X_rfc_woNaN[~np.isnan(y_rfc)[:,0], :])\n",
    "y_rfc_woNaN = y_rfc[~np.isnan(y_rfc)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_rfc_train, X_rfc_test, y_rfc_train, y_rfc_test = train_test_split(X_rfc_woNaN, y_rfc_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_rfc = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', RandomForestClassifier())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_rfc.fit(X_rfc_train, y_rfc_train)\n",
    "\n",
    "# Predicting values\n",
    "y_rfc_pred = pipe_rfc.predict(X_rfc_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_rfc_pred, y_rfc_test)\n",
    "print('Accuracy score for random forest classifier : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"random forest\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1f85e4",
   "metadata": {},
   "source": [
    "## 3.6 Model 6 : Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cf9014",
   "metadata": {},
   "source": [
    "### 3.6.1 Normal one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd86582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SVC\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_svmn = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_svmn = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_svmn_woNaN = imputer.fit_transform(X_svmn)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_svmn_woNaN = (X_svmn_woNaN[~np.isnan(y_svmn)[:,0], :])\n",
    "y_svmn_woNaN = y_svmn[~np.isnan(y_svmn)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_svmn_train, X_svmn_test, y_svmn_train, y_svmn_test = train_test_split(X_svmn_woNaN, y_svmn_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_svmn = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', svm.SVC())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_svmn.fit(X_svmn_train, y_svmn_train)\n",
    "\n",
    "# Predicting values\n",
    "y_svmn_pred = pipe_svmn.predict(X_svmn_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_svmn_pred, y_svmn_test)\n",
    "print('Accuracy score for normal SVC : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"normal SVM classification\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ffbc40",
   "metadata": {},
   "source": [
    "### 3.6.2 Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b505510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform LinearSVM\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_svml = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_svml = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_svml_woNaN = imputer.fit_transform(X_svml)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_svml_woNaN = (X_svml_woNaN[~np.isnan(y_svml)[:,0], :])\n",
    "y_svml_woNaN = y_svml[~np.isnan(y_svml)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_svml_train, X_svml_test, y_svml_train, y_svml_test = train_test_split(X_svml_woNaN, y_svml_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_svml = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', svm.LinearSVC())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_svml.fit(X_svml_train, y_svml_train)\n",
    "\n",
    "# Predicting values\n",
    "y_svml_pred = pipe_svml.predict(X_svml_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_svml_pred, y_svml_test)\n",
    "print('Accuracy score for linear SVC : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"linear SVM classification\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693cf4c3",
   "metadata": {},
   "source": [
    "## 3.7 Model 7 : Multi-layer perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0077a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform MLPClassifier\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_mlp = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_mlp = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_mlp_woNaN = imputer.fit_transform(X_mlp)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_mlp_woNaN = (X_mlp_woNaN[~np.isnan(y_mlp)[:,0], :])\n",
    "y_mlp_woNaN = y_mlp[~np.isnan(y_mlp)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_mlp_train, X_mlp_test, y_mlp_train, y_mlp_test = train_test_split(X_mlp_woNaN, y_mlp_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_mlp = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', MLPClassifier())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_mlp.fit(X_mlp_train, y_mlp_train)\n",
    "\n",
    "# Predicting values\n",
    "y_mlp_pred = pipe_mlp.predict(X_mlp_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_mlp_pred, y_mlp_test)\n",
    "print('Accuracy score for MLP Classifier : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"MLP classifier\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a77bf0d",
   "metadata": {},
   "source": [
    "## 3.8 Model 8 : Decision tree classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09785ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform DecisionTreeClassifier\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_dtc = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_dtc = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_dtc_woNaN = imputer.fit_transform(X_dtc)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_dtc_woNaN = (X_dtc_woNaN[~np.isnan(y_dtc)[:,0], :])\n",
    "y_dtc_woNaN = y_dtc[~np.isnan(y_dtc)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_dtc_train, X_dtc_test, y_dtc_train, y_dtc_test = train_test_split(X_dtc_woNaN, y_dtc_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_dtc = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', DecisionTreeClassifier())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_dtc.fit(X_dtc_train, y_dtc_train)\n",
    "\n",
    "# Predicting values\n",
    "y_dtc_pred = pipe_dtc.predict(X_dtc_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_dtc_pred, y_dtc_test)\n",
    "print('Accuracy score for Decision Tree classifier : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"decision tree classifier\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b4d24a",
   "metadata": {},
   "source": [
    "## 3.9 Model 9 : ADABoost Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6f44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ADABoostClassifier\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_abc = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_abc = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_abc_woNaN = imputer.fit_transform(X_abc)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_abc_woNaN = (X_abc_woNaN[~np.isnan(y_abc)[:,0], :])\n",
    "y_abc_woNaN = y_abc[~np.isnan(y_abc)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_abc_train, X_abc_test, y_abc_train, y_abc_test = train_test_split(X_abc_woNaN, y_abc_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_abc = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', DecisionTreeClassifier())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_abc.fit(X_abc_train, y_abc_train)\n",
    "\n",
    "# Predicting values\n",
    "y_abc_pred = pipe_abc.predict(X_abc_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_abc_pred, y_abc_test)\n",
    "print('Accuracy score for ADABoost classifier : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"ADABoost classifier\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a430ba6",
   "metadata": {},
   "source": [
    "## 3.10 Model 10 : Extra trees classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a603572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ExtraTreesClassifier\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_etc = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_etc = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_etc_woNaN = imputer.fit_transform(X_etc)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_etc_woNaN = (X_etc_woNaN[~np.isnan(y_etc)[:,0], :])\n",
    "y_etc_woNaN = y_etc[~np.isnan(y_etc)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_etc_train, X_etc_test, y_etc_train, y_etc_test = train_test_split(X_etc_woNaN, y_etc_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_etc = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', ExtraTreesClassifier())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_etc.fit(X_etc_train, y_etc_train)\n",
    "\n",
    "# Predicting values\n",
    "y_etc_pred = pipe_etc.predict(X_etc_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_etc_pred, y_etc_test)\n",
    "print('Accuracy score for Extra Trees classifier : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"extra trees classifier\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f2453",
   "metadata": {},
   "source": [
    "## 3.11 Model 11 : Discriminant analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db31a77",
   "metadata": {},
   "source": [
    "### 3.11.1 Linear discriminant analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df63124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform LinearDiscriminantAnalysis\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_lda = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_lda = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_lda_woNaN = imputer.fit_transform(X_lda)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_lda_woNaN = (X_lda_woNaN[~np.isnan(y_lda)[:,0], :])\n",
    "y_lda_woNaN = y_lda[~np.isnan(y_lda)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_lda_train, X_lda_test, y_lda_train, y_lda_test = train_test_split(X_lda_woNaN, y_lda_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_lda = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', LinearDiscriminantAnalysis())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_lda.fit(X_lda_train, y_lda_train)\n",
    "\n",
    "# Predicting values\n",
    "y_lda_pred = pipe_lda.predict(X_lda_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_lda_pred, y_lda_test)\n",
    "print('Accuracy score for Linear Dicriminant Analysis : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"linear discriminant analysis\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a018f3d",
   "metadata": {},
   "source": [
    "### 3.11.2 Quadratic discriminant analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303ab372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform QuadraticDiscriminantAnalysis\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_qda = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_qda = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_qda_woNaN = imputer.fit_transform(X_qda)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_qda_woNaN = (X_qda_woNaN[~np.isnan(y_qda)[:,0], :])\n",
    "y_qda_woNaN = y_qda[~np.isnan(y_qda)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_qda_train, X_qda_test, y_qda_train, y_qda_test = train_test_split(X_qda_woNaN, y_qda_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_qda = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', QuadraticDiscriminantAnalysis())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_qda.fit(X_qda_train, y_qda_train)\n",
    "\n",
    "# Predicting values\n",
    "y_qda_pred = pipe_qda.predict(X_qda_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_qda_pred, y_qda_test)\n",
    "print('Accuracy score for Quadratic Discriminant Analysis : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"quadratic discriminant classifier\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618e2ed5",
   "metadata": {},
   "source": [
    "## 3.12 Model 12 : Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb8281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SGDClassifier\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_sgd = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_sgd = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_sgd_woNaN = imputer.fit_transform(X_sgd)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_sgd_woNaN = (X_sgd_woNaN[~np.isnan(y_sgd)[:,0], :])\n",
    "y_sgd_woNaN = y_sgd[~np.isnan(y_sgd)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_sgd_train, X_sgd_test, y_sgd_train, y_sgd_test = train_test_split(X_sgd_woNaN, y_sgd_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_sgd = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', SGDClassifier())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_sgd.fit(X_sgd_train, y_sgd_train)\n",
    "\n",
    "# Predicting values\n",
    "y_sgd_pred = pipe_sgd.predict(X_sgd_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_sgd_pred, y_sgd_test)\n",
    "print('Accuracy score for Stochastic gradient descent classifier : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"stochastic gradient descent\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acf8df0",
   "metadata": {},
   "source": [
    "## 3.13 Model 13 : XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea24cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform XGBClassifier\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_xgb = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_xgb = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_xgb_woNaN = imputer.fit_transform(X_xgb)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_xgb_woNaN = (X_xgb_woNaN[~np.isnan(y_xgb)[:,0], :])\n",
    "y_xgb_woNaN = y_xgb[~np.isnan(y_xgb)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_xgb_train, X_xgb_test, y_xgb_train, y_xgb_test = train_test_split(X_xgb_woNaN, y_xgb_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_xgb = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42))])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_xgb.fit(X_xgb_train, y_xgb_train)\n",
    "\n",
    "# Predicting values\n",
    "y_xgb_pred = pipe_xgb.predict(X_xgb_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_xgb_pred, y_xgb_test)\n",
    "print('Accuracy score for XGBoost classifier : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"XGBoost classifier\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722e2f54",
   "metadata": {},
   "source": [
    "## 3.14 Model 14 : Gaussian process classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b1b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform GaussianProcessClassifier\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_gpc = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_gpc = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_gpc_woNaN = imputer.fit_transform(X_gpc)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_gpc_woNaN = (X_gpc_woNaN[~np.isnan(y_gpc)[:,0], :])\n",
    "y_gpc_woNaN = y_gpc[~np.isnan(y_gpc)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_gpc_train, X_gpc_test, y_gpc_train, y_gpc_test = train_test_split(X_gpc_woNaN, y_gpc_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_gpc = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', GaussianProcessClassifier())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_gpc.fit(X_gpc_train, y_gpc_train)\n",
    "\n",
    "# Predicting values\n",
    "y_gpc_pred = pipe_gpc.predict(X_gpc_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_gpc_pred, y_gpc_test)\n",
    "print('Accuracy score for Gaussian process classifier : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"gaussian process classifier\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21869b1",
   "metadata": {},
   "source": [
    "## 3.15 Model 15 : Passive aggressive classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3544f5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PassiveAggressiveClassifier\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_pac = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_pac = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_pac_woNaN = imputer.fit_transform(X_pac)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_pac_woNaN = (X_pac_woNaN[~np.isnan(y_pac)[:,0], :])\n",
    "y_pac_woNaN = y_pac[~np.isnan(y_pac)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_pac_train, X_pac_test, y_pac_train, y_pac_test = train_test_split(X_pac_woNaN, y_pac_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_pac = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', PassiveAggressiveClassifier())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_pac.fit(X_pac_train, y_pac_train)\n",
    "\n",
    "# Predicting values\n",
    "y_pac_pred = pipe_pac.predict(X_pac_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_pac_pred, y_pac_test)\n",
    "print('Accuracy score for Gaussian process classifier : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"passive aggressive classifier\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f242319",
   "metadata": {},
   "source": [
    "## 3.16 Model 16 : Linear perceptron classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce86f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Perceptron\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_lpc = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_lpc = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_lpc_woNaN = imputer.fit_transform(X_lpc)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_lpc_woNaN = (X_lpc_woNaN[~np.isnan(y_lpc)[:,0], :])\n",
    "y_lpc_woNaN = y_lpc[~np.isnan(y_lpc)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_lpc_train, X_lpc_test, y_lpc_train, y_lpc_test = train_test_split(X_lpc_woNaN, y_lpc_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_lpc = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', Perceptron())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_lpc.fit(X_lpc_train, y_lpc_train)\n",
    "\n",
    "# Predicting values\n",
    "y_lpc_pred = pipe_lpc.predict(X_lpc_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_lpc_pred, y_lpc_test)\n",
    "print('Accuracy score for Gaussian process classifier : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"perceptron\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfad584",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.sort_values(by = \"accuracy\", axis = 0, ascending  = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
