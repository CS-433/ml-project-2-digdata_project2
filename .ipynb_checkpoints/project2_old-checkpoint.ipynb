{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7161f50f-14de-4a00-9951-535df873c0d2",
   "metadata": {},
   "source": [
    "# Project 2: Covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea38c3b3-4b5c-4346-a021-66fd167ed5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9743d5-e58e-4dd8-b8b2-7a4eed760c9b",
   "metadata": {},
   "source": [
    "## 1. Literature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae98ebb-8302-4241-b8f5-8c14173f3453",
   "metadata": {},
   "source": [
    "In 2019, the first COVID-19 cases are observed in China. Rapidly, the SARS-Cov2 virus spread worldwide, pushing governments to take strict decisions about the lives of their co-citizens, like containment, to protect the population. Indeed, in some cases, COVID-19 patients ended up in intensive care services and sometimes died.\n",
    "\n",
    "The aim of our model is, based on easily computable parameters at the study's beginning, to predict whether the patient will be likely to die or if the chance of survival is important. The point of this study is to help the hospital organise in the case of a high number of cases.\n",
    "\n",
    "\n",
    "The studied dataset stem from the IDDO Data Repository of COVID-19 data. This data was pulled from the underlying data collection projects on 2022-09-01. The data comes from 1,200 institutions from over 45 countries and gather various information from 700,000 hospitalised individuals.\n",
    "\n",
    "To keep only the relevant features, we first dive into the literature, using Meta-analysis papers. First, we have been looking for aggravating factors that will likely lead the patient to ICU.\n",
    "\n",
    "Obesity: according to a meta-analysis by Sales-Peres, there is a correlation between obesity and ICU admission. This paper also concluded that co-morbidities for obese patients, such as hypertension, type 2 diabetes, smoking habit, lung disease, and/or cardiovascular disease lead to a higher chance of ICU admission.\n",
    "Age: patients aged 70 years and above have a higher risk of infection and a higher need for intensive care than patients younger than 70.\n",
    "Sex: men, when infected, have a higher risk of severe COVID-19 disease and a higher need for intensive care than women\\cite{pijls_demographic_2021}.\n",
    "Ethnicity: the risk of contamination was higher in most ethnic minority groups than their White counterparts in North America and Europe. Among people with confirmed infection, African-Americans and Hispanic Americans were also more likely than White Americans to be hospitalised with SARS-CoV-2 infection. However, the probability of ICU admission was equivalent for all groups. Thus, ethnicity is not relevant to our question. \n",
    "Blood tests: Patients with increased pancreatic enzymes, including elevated serum lipase or amylase of either type, had worse clinical outcomes. Lower levels of lymphocytes and hemoglobin; elevated levels of leukocytes, aspartate aminotransferase, alanine aminotransferase, blood creatinine, blood urea nitrogen, high-sensitivity troponin, creatine kinase, high-sensitivity C-reactive protein, interleukin 6, D-dimer, ferritin, lactate dehydrogenase, and procalcitonin; and a high erythrocyte sedimentation rate were also associated with severe COVID-19.  \n",
    "\n",
    "Out of a total of 3009 citations, 17 articles (22 studies, 21 from China and one study from Singapore) with 3396 ranging from 12 to1099 patients were included. Our meta-analyses showed a significant decrease in lymphocyte, monocyte, and eosinophil, hemoglobin, platelet, albumin, serum sodium, lymphocyte to C-reactive protein ratio (LCR), leukocyte to C-reactive protein ratio (LeCR), leukocyte to IL-6 ratio (LeIR), and an increase in the neutrophil, alanine aminotransferase (ALT), aspartate aminotransferase (AST), total bilirubin, blood urea nitrogen (BUN), creatinine (Cr), erythrocyte Sedimentation Rate (ESR), C-reactive protein (CRP), Procalcitonin (PCT), lactate dehydrogenase (LDH), fibrinogen, prothrombin time (PT), D-dimer, glucose level, and neutrophil to lymphocyte ratio (NLR) in the severe group compared with the non-severe group. \n",
    "\n",
    "No significant changes in white blood cells (WBC), Creatine Kinase (CK), troponin I, myoglobin, IL-6 and K between the two groups were observed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9250aa15-370f-4871-af7b-abe871e57467",
   "metadata": {},
   "source": [
    "## 2. Load data that come from data_selection file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b89e6596-eb3c-4383-9d82-b4cf9f55e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file\n",
    "data_folder = './data/results/'\n",
    "mylist = []\n",
    "for chunk in pd.read_csv(data_folder + 'df.csv', sep=',', low_memory=False, chunksize=5000):\n",
    "    mylist.append(chunk)\n",
    "df = pd.concat(mylist, axis=0)\n",
    "df.name = 'df'\n",
    "del mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fed57e35-089f-4979-bbaa-e4e657a24d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete row where DSDECOD is NA\n",
    "df = df[df.DSDECOD != np.nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a036707-4fe0-4a76-8b8c-691d507885ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INCLAS_DRUGS_FOR_OBSTRUCTIVE_AIRWAY_DISEASES        0.0\n",
       "INCLAS_EXTRACORPOREAL_MEMBRANE_OXYGENATION          1.0\n",
       "INCLAS_HIGH_FLOW_OXYGEN_NASAL_CANNULA               1.0\n",
       "INCLAS_IMMUNOGLOBULINS                              0.0\n",
       "INCLAS_IMMUNOSTIMULANTS                             0.0\n",
       "INCLAS_IMMUNOSUPPRESSANTS                           1.0\n",
       "INCLAS_INSERTION_OF_TRACHEOSTOMY_TUBE               1.0\n",
       "INCLAS_INTUBATION                                   0.0\n",
       "INCLAS_LIPID_MODIFYING_AGENTS                       0.0\n",
       "INCLAS_MUSCLE_RELAXANTS                             0.0\n",
       "INCLAS_NONINVASIVE_POSITIVE_PRESSURE_VENTILATION    0.0\n",
       "INCLAS_NONINVASIVE_VENTILATION                      1.0\n",
       "INCLAS_OTHER_RESPIRATORY_SYSTEM_PRODUCTS            1.0\n",
       "INCLAS_OXYGEN                                       1.0\n",
       "INCLAS_PERCUTANEOUS_ENDOSCOPIC_GASTROSTOMY          0.0\n",
       "INCLAS_PRONE_BODY_POSITION                          1.0\n",
       "INCLAS_PSYCHOLEPTICS                                0.0\n",
       "INCLAS_REMOVAL_OF_ENDOTRACHEAL_TUBE                 0.0\n",
       "INCLAS_RENAL_REPLACEMENT                            1.0\n",
       "INCLAS_REPLACEMENT_AGENT                            0.0\n",
       "INCLAS_TOTAL_PARENTERAL_NUTRITION                   0.0\n",
       "INCLAS_TRANSFUSION_OF_BLOOD_PRODUCT                 0.0\n",
       "INCLAS_VACCINES                                     0.0\n",
       "LBTEST_ALT                                           <7\n",
       "LBTEST_APTT                                         NaN\n",
       "LBTEST_APTTSTND                                     NaN\n",
       "LBTEST_AST                                          NaN\n",
       "LBTEST_BILI                                          <3\n",
       "LBTEST_CD4                                          NaN\n",
       "LBTEST_CREAT                                         75\n",
       "LBTEST_CRP                                          189\n",
       "Name: 914, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[914, 29:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a1e3fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deal with '<'\n",
    "data_test = df\n",
    "data_test =data_test.astype('string')\n",
    "\n",
    "for (colname, colval) in data_test.iteritems():\n",
    "    data = pd.DataFrame()\n",
    "    data[colname] = data_test[colname].str.find('<')\n",
    "    data = data.dropna()\n",
    "    data = data[data != -1]\n",
    "    \n",
    "    indexes = data.index\n",
    "    data_test[colname]= data_test[colname].loc[indexes].apply(lambda x: x.replace('<', ''))\n",
    "    df[colname] = data_test[colname]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7b6b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deal with '>'\n",
    "data_test = df\n",
    "data_test =data_test.astype('string')\n",
    "\n",
    "for (colname, colval) in data_test.iteritems():\n",
    "    data = pd.DataFrame()\n",
    "    data[colname] = data_test[colname].str.find('>')\n",
    "    data = data.dropna()\n",
    "    data = data[data != -1]\n",
    "    \n",
    "    indexes = data.index\n",
    "    data_test[colname]= data_test[colname].loc[indexes].apply(lambda x: x.replace('>', ''))\n",
    "    df[colname] = data_test[colname]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "140106ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deal with '''\n",
    "data_test = df\n",
    "data_test =data_test.astype('string')\n",
    "\n",
    "for (colname, colval) in data_test.iteritems():\n",
    "    data = pd.DataFrame()\n",
    "    data[colname] = data_test[colname].str.find(\"'\")\n",
    "    data = data.dropna()\n",
    "    data = data[data != -1]\n",
    "    \n",
    "    indexes = data.index\n",
    "    data_test[colname]= data_test[colname].loc[indexes].apply(lambda x: x.replace(\"'\", '.'))\n",
    "    df[colname] = data_test[colname]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abee6b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('LBTEST_WBC', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5687ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deal with '='\n",
    "data_test = df\n",
    "data_test =data_test.astype('string')\n",
    "\n",
    "for (colname, colval) in data_test.iteritems():\n",
    "    data = pd.DataFrame()\n",
    "    data[colname] = data_test[colname].str.find('=')\n",
    "    data = data.dropna()\n",
    "    data = data[data != -1]\n",
    "    \n",
    "    indexes = data.index\n",
    "    data_test[colname]= data_test[colname].loc[indexes].apply(lambda x: x.replace('=', ''))\n",
    "    df[colname] = data_test[colname]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0542e349-7541-4e59-bf90-f3ed4b585bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2321db7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LBTEST_PT          NaN\n",
       "LBTEST_SODIUM      NaN\n",
       "LBTEST_UREAN       NaN\n",
       "MBTEST_ADENOVIR    NaN\n",
       "MBTEST_BACT        NaN\n",
       "MBTEST_CRONAVIR    1.0\n",
       "MBTEST_INFLUVIR    NaN\n",
       "MBTEST_RSV         NaN\n",
       "RPSTRESC           0.0\n",
       "RSCAT_AVPU         NaN\n",
       "Name: 246580, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[246580, 70:80]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b730981c",
   "metadata": {},
   "source": [
    "# 3. Testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c68612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ade6658-bec2-4d54-8557-00d697033f7f",
   "metadata": {},
   "source": [
    "## 3.1 Model 1: Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7e3b2b-fa29-4efc-acbc-e651ba631be6",
   "metadata": {},
   "source": [
    "### 3.1.1 Basic method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f26bf485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9582, 91) (9582,)\n"
     ]
    }
   ],
   "source": [
    "# Perform Logistic regression\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "\n",
    "X_reg = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_reg = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# X_reg = df.loc[:, df.columns != 'DSDECOD'].to_numpy()\n",
    "# y_reg = df['DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_reg_woNaN = imputer.fit_transform(X_reg)\n",
    "\n",
    "# Get rid of rows with missing y values \n",
    "X_reg_woNaN = (X_reg_woNaN[~np.isnan(y_reg)[:,0], :])\n",
    "y_reg_woNaN = y_reg[~np.isnan(y_reg)]\n",
    "\n",
    "print(X_reg_woNaN.shape, y_reg_woNaN.shape)\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg_woNaN, y_reg_woNaN, test_size=0.3, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6599f925-4e1d-4dad-8129-bfd8ac822b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for logistic regression :  0.7756521739130435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_reg = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', LogisticRegression())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_reg.fit(X_reg_train, y_reg_train)\n",
    "\n",
    "# Predicting values\n",
    "y_reg_pred = pipe_reg.predict(X_reg_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_reg_pred, y_reg_test)\n",
    "print('Accuracy score for logistic regression : ',acc_score)\n",
    "\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"logistic regression\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccb8beed-ff36-4bf7-8e12-ae1c87cd7788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0305064 , -0.0305064 ,  0.0631653 ,  1.00901016,  0.10331251,\n",
       "        -0.43363289,  0.05384158,  0.42824211, -0.03452648,  0.01237673,\n",
       "         0.11993767,  0.02247719, -0.40736083, -0.17142362, -0.07469627,\n",
       "         0.09040787,  0.03281372,  0.14527892,  0.06458806,  0.07619566,\n",
       "         0.05194113,  0.06298494,  0.10629646,  0.06298494,  0.25495093,\n",
       "        -0.11743616,  0.04109685,  0.02852484,  0.00991371, -0.0976772 ,\n",
       "        -0.10283493,  0.06298494,  0.25495093,  0.06033379, -0.36045442,\n",
       "         0.        , -0.08428407,  0.01132118,  0.        , -0.03293556,\n",
       "         0.13605213, -0.02072253,  0.        ,  0.26670927,  0.06850497,\n",
       "         0.05194113, -0.31628739, -0.06510055,  0.        ,  0.02819887,\n",
       "         0.06594656,  0.04030056,  0.01442919,  0.00309555,  0.03186287,\n",
       "        -0.02892186, -0.02278356,  0.0428727 ,  0.10340595, -0.01456693,\n",
       "         0.01367133, -0.01804781,  0.0103535 ,  0.01502997,  0.00934013,\n",
       "        -0.05558385, -0.01979175,  0.08210517, -0.06689241, -0.00272497,\n",
       "        -0.06210187,  0.00934818,  0.02344183, -0.03859543, -0.02735382,\n",
       "         0.05651719,  0.        , -0.18738679,  0.21706062, -0.03459087,\n",
       "         0.10424951,  0.20718056,  0.05060472,  0.        , -0.03753464,\n",
       "         0.08343147,  0.0189136 , -0.0627779 ,  0.09015478, -0.03483892,\n",
       "        -0.04109017]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See coeffs of the model\n",
    "pipe_reg.named_steps['clf'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cd4544-deca-40fa-aa3a-85864d34353d",
   "metadata": {},
   "source": [
    "### 3.1.2 Play with logistic regression parameters, cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c0e6d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['C', 'class_weight', 'dual', 'fit_intercept', 'intercept_scaling', 'l1_ratio', 'max_iter', 'multi_class', 'n_jobs', 'penalty', 'random_state', 'solver', 'tol', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa60bd85-2372-4183-aa3e-d77f917307ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 4 is smaller than n_iter=30. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for logistic regression :  0.7756521739130435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.78932531        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\sande\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2176/1980668042.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy score for logistic regression : '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0macc_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "# Perform Logistic regression with cross-validation\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_reg = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_reg = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_reg_woNaN = imputer.fit_transform(X_reg)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_reg_woNaN = (X_reg_woNaN[~np.isnan(y_reg)[:,0], :])\n",
    "y_reg_woNaN = y_reg[~np.isnan(y_reg)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg_woNaN, y_reg_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_reg = Pipeline([('scl', StandardScaler()), ('clf', LogisticRegression())])\n",
    "\n",
    "# Set parameters to test\n",
    "param_reg = {'clf__penalty': [None, 'l1', 'l2', 'elasticnet']}\n",
    "\n",
    "# Cross-validation\n",
    "cv_reg = RandomizedSearchCV(estimator = pipe_reg, \n",
    "                                         param_distributions=param_reg, \n",
    "                                         cv=3, n_iter=30, n_jobs=-1)\n",
    "\n",
    "# Fit data into the model\n",
    "cv_reg.fit(X_reg_train, y_reg_train)\n",
    "\n",
    "# Predicting values\n",
    "y_reg_pred = cv_reg.predict(X_reg_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_reg_pred, y_reg_test)\n",
    "print('Accuracy score for logistic regression : ',acc_score)\n",
    "\n",
    "clf.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db92e6b1-ec63-4085-b34d-d901e251838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See best parameters of the model\n",
    "g_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af7203b-6f23-4968-8990-d8dc685b6f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See coeffs of the model\n",
    "cv_reg.named_steps['clf'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f809a66e-2d05-4fa0-a610-8bb9f430c68f",
   "metadata": {},
   "source": [
    "## 3.2 Model 2: GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6116709-d110-41d1-b5c5-2c118b69d7bd",
   "metadata": {},
   "source": [
    "Estimators that allow NaN values for type classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "88c8dc25-a3e1-42c0-96e0-9324393eeae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for gradient boosting classification :  0.7937391304347826\n"
     ]
    }
   ],
   "source": [
    "# Perform GradientBoostingClassifier\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_hgbc = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_hgbc = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_hgbc_woNaN = imputer.fit_transform(X_hgbc)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_hgbc_woNaN = (X_hgbc_woNaN[~np.isnan(y_hgbc)[:,0], :])\n",
    "y_hgbc_woNaN = y_hgbc[~np.isnan(y_hgbc)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_hgbc_train, X_hgbc_test, y_hgbc_train, y_hgbc_test = train_test_split(X_hgbc_woNaN, y_hgbc_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_hgbc = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', GradientBoostingClassifier())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_hgbc.fit(X_hgbc_train, y_hgbc_train)\n",
    "\n",
    "# Predicting values\n",
    "y_hgbc_pred = pipe_hgbc.predict(X_hgbc_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_hgbc_pred, y_hgbc_test)\n",
    "print('Accuracy score for gradient boosting classification : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"gradient boosting classifier\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13710c8b",
   "metadata": {},
   "source": [
    "## 3.3 Model 3 : K-Nearest Neighbor Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "50dabc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for k-nearest neighbor classification :  0.7638260869565218\n"
     ]
    }
   ],
   "source": [
    "# Perform KNeighborsClassifier\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_knn = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_knn = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_knn_woNaN = imputer.fit_transform(X_knn)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_knn_woNaN = (X_knn_woNaN[~np.isnan(y_knn)[:,0], :])\n",
    "y_knn_woNaN = y_knn[~np.isnan(y_knn)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_knn_train, X_knn_test, y_knn_train, y_knn_test = train_test_split(X_knn_woNaN, y_knn_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_knn = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', KNeighborsClassifier())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_knn.fit(X_knn_train, y_knn_train)\n",
    "\n",
    "# Predicting values\n",
    "y_knn_pred = pipe_knn.predict(X_knn_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_knn_pred, y_knn_test)\n",
    "print('Accuracy score for k-nearest neighbor classification : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"k-nearest neighbors\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6820d6b8",
   "metadata": {},
   "source": [
    "## 3.4 Model 4 : Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "32c28b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for naive Bayes :  0.7714782608695652\n"
     ]
    }
   ],
   "source": [
    "# Perform GaussianNB\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_gnb = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_gnb = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_gnb_woNaN = imputer.fit_transform(X_gnb)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_gnb_woNaN = (X_gnb_woNaN[~np.isnan(y_gnb)[:,0], :])\n",
    "y_gnb_woNaN = y_gnb[~np.isnan(y_gnb)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_gnb_train, X_gnb_test, y_gnb_train, y_gnb_test = train_test_split(X_gnb_woNaN, y_gnb_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_gnb = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', GaussianNB())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_gnb.fit(X_gnb_train, y_gnb_train)\n",
    "\n",
    "# Predicting values\n",
    "y_gnb_pred = pipe_gnb.predict(X_gnb_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_gnb_pred, y_gnb_test)\n",
    "print('Accuracy score for naive Bayes : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"naive Bayes\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b1a9e",
   "metadata": {},
   "source": [
    "## 3.5 Model 5 : Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e8e7f3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for random forest classifier :  0.7707826086956522\n"
     ]
    }
   ],
   "source": [
    "# Perform RandomForestClassifier\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_rfc = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_rfc = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_rfc_woNaN = imputer.fit_transform(X_rfc)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_rfc_woNaN = (X_rfc_woNaN[~np.isnan(y_rfc)[:,0], :])\n",
    "y_rfc_woNaN = y_rfc[~np.isnan(y_rfc)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_rfc_train, X_rfc_test, y_rfc_train, y_rfc_test = train_test_split(X_rfc_woNaN, y_rfc_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_rfc = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', RandomForestClassifier())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_rfc.fit(X_rfc_train, y_rfc_train)\n",
    "\n",
    "# Predicting values\n",
    "y_rfc_pred = pipe_rfc.predict(X_rfc_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_rfc_pred, y_rfc_test)\n",
    "print('Accuracy score for random forest classifier : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"random forest\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1f85e4",
   "metadata": {},
   "source": [
    "## 3.6 Model 6 : Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cf9014",
   "metadata": {},
   "source": [
    "### 3.6.1 Normal one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2cd86582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for normal SVC :  0.792\n"
     ]
    }
   ],
   "source": [
    "# Perform SVC\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_svmn = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_svmn = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_svmn_woNaN = imputer.fit_transform(X_svmn)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_svmn_woNaN = (X_svmn_woNaN[~np.isnan(y_svmn)[:,0], :])\n",
    "y_svmn_woNaN = y_svmn[~np.isnan(y_svmn)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_svmn_train, X_svmn_test, y_svmn_train, y_svmn_test = train_test_split(X_svmn_woNaN, y_svmn_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_svmn = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', svm.SVC())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_svmn.fit(X_svmn_train, y_svmn_train)\n",
    "\n",
    "# Predicting values\n",
    "y_svmn_pred = pipe_svmn.predict(X_svmn_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_svmn_pred, y_svmn_test)\n",
    "print('Accuracy score for normal SVC : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"normal SVM classification\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ffbc40",
   "metadata": {},
   "source": [
    "### 3.6.2 Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b505510e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for linear SVC :  0.7843478260869565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "# Perform LinearSVM\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_svml = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_svml = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_svml_woNaN = imputer.fit_transform(X_svml)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_svml_woNaN = (X_svml_woNaN[~np.isnan(y_svml)[:,0], :])\n",
    "y_svml_woNaN = y_svml[~np.isnan(y_svml)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_svml_train, X_svml_test, y_svml_train, y_svml_test = train_test_split(X_svml_woNaN, y_svml_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_svml = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', svm.LinearSVC())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_svml.fit(X_svml_train, y_svml_train)\n",
    "\n",
    "# Predicting values\n",
    "y_svml_pred = pipe_svml.predict(X_svml_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_svml_pred, y_svml_test)\n",
    "print('Accuracy score for linear SVC : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"linear SVM classification\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693cf4c3",
   "metadata": {},
   "source": [
    "## 3.7 Model 7 : Multi-layer perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0077a9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for MLP Classifier :  0.782608695652174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "# Perform MLPClassifier\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_mlp = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_mlp = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_mlp_woNaN = imputer.fit_transform(X_mlp)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_mlp_woNaN = (X_mlp_woNaN[~np.isnan(y_mlp)[:,0], :])\n",
    "y_mlp_woNaN = y_mlp[~np.isnan(y_mlp)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_mlp_train, X_mlp_test, y_mlp_train, y_mlp_test = train_test_split(X_mlp_woNaN, y_mlp_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_mlp = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', svm.LinearSVC())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_mlp.fit(X_mlp_train, y_mlp_train)\n",
    "\n",
    "# Predicting values\n",
    "y_mlp_pred = pipe_mlp.predict(X_mlp_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_mlp_pred, y_mlp_test)\n",
    "print('Accuracy score for MLP Classifier : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"MLP classifier\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a77bf0d",
   "metadata": {},
   "source": [
    "## 3.8 Model 8 : Decision tree classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "09785ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for Decision Tree classifier :  0.72\n"
     ]
    }
   ],
   "source": [
    "# Perform DecisionTreeClassifier\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_dtc = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_dtc = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_dtc_woNaN = imputer.fit_transform(X_dtc)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_dtc_woNaN = (X_dtc_woNaN[~np.isnan(y_dtc)[:,0], :])\n",
    "y_dtc_woNaN = y_dtc[~np.isnan(y_dtc)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_dtc_train, X_dtc_test, y_dtc_train, y_dtc_test = train_test_split(X_dtc_woNaN, y_dtc_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_dtc = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', DecisionTreeClassifier())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_dtc.fit(X_dtc_train, y_dtc_train)\n",
    "\n",
    "# Predicting values\n",
    "y_dtc_pred = pipe_dtc.predict(X_dtc_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_dtc_pred, y_dtc_test)\n",
    "print('Accuracy score for Decision Tree classifier : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"decision tree classifier\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b4d24a",
   "metadata": {},
   "source": [
    "## 3.9 Model 9 : ADABoost Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fb6f44fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for ADABoost classifier :  0.7109565217391305\n"
     ]
    }
   ],
   "source": [
    "# Perform ADABoostClassifier\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_abc = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_abc = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_abc_woNaN = imputer.fit_transform(X_abc)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_abc_woNaN = (X_abc_woNaN[~np.isnan(y_abc)[:,0], :])\n",
    "y_abc_woNaN = y_abc[~np.isnan(y_abc)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_abc_train, X_abc_test, y_abc_train, y_abc_test = train_test_split(X_abc_woNaN, y_abc_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_abc = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', DecisionTreeClassifier())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_abc.fit(X_abc_train, y_abc_train)\n",
    "\n",
    "# Predicting values\n",
    "y_abc_pred = pipe_abc.predict(X_abc_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_abc_pred, y_abc_test)\n",
    "print('Accuracy score for ADABoost classifier : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"ADABoost classifier\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a430ba6",
   "metadata": {},
   "source": [
    "## 3.10 Model 10 : Extra trees classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0a603572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for Extra Trees classifier :  0.7718260869565218\n"
     ]
    }
   ],
   "source": [
    "# Perform ExtraTreesClassifier\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_etc = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_etc = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_etc_woNaN = imputer.fit_transform(X_etc)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_etc_woNaN = (X_etc_woNaN[~np.isnan(y_etc)[:,0], :])\n",
    "y_etc_woNaN = y_etc[~np.isnan(y_etc)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_etc_train, X_etc_test, y_etc_train, y_etc_test = train_test_split(X_etc_woNaN, y_etc_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_etc = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', ExtraTreesClassifier())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_etc.fit(X_etc_train, y_etc_train)\n",
    "\n",
    "# Predicting values\n",
    "y_etc_pred = pipe_etc.predict(X_etc_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_etc_pred, y_etc_test)\n",
    "print('Accuracy score for Extra Trees classifier : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"extra trees classifier\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f2453",
   "metadata": {},
   "source": [
    "## 3.11 Model 11 : Discriminant analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db31a77",
   "metadata": {},
   "source": [
    "### 3.11.1 Linear discriminant analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1df63124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for Linear Dicriminant Analysis :  0.7791304347826087\n"
     ]
    }
   ],
   "source": [
    "# Perform LinearDiscriminantAnalysis\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_lda = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_lda = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_lda_woNaN = imputer.fit_transform(X_lda)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_lda_woNaN = (X_lda_woNaN[~np.isnan(y_lda)[:,0], :])\n",
    "y_lda_woNaN = y_lda[~np.isnan(y_lda)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_lda_train, X_lda_test, y_lda_train, y_lda_test = train_test_split(X_lda_woNaN, y_lda_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_lda = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', LinearDiscriminantAnalysis())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_lda.fit(X_lda_train, y_lda_train)\n",
    "\n",
    "# Predicting values\n",
    "y_lda_pred = pipe_lda.predict(X_lda_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_lda_pred, y_lda_test)\n",
    "print('Accuracy score for Linear Dicriminant Analysis : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"linear discriminant analysis\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a018f3d",
   "metadata": {},
   "source": [
    "### 3.11.2 Quadratic discriminant analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "303ab372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for Quadratic Discriminant Analysis :  0.2831304347826087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "# Perform QuadraticDiscriminantAnalysis\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_qda = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_qda = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_qda_woNaN = imputer.fit_transform(X_qda)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_qda_woNaN = (X_qda_woNaN[~np.isnan(y_qda)[:,0], :])\n",
    "y_qda_woNaN = y_qda[~np.isnan(y_qda)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_qda_train, X_qda_test, y_qda_train, y_qda_test = train_test_split(X_qda_woNaN, y_qda_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_qda = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', QuadraticDiscriminantAnalysis())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_qda.fit(X_qda_train, y_qda_train)\n",
    "\n",
    "# Predicting values\n",
    "y_qda_pred = pipe_qda.predict(X_qda_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_qda_pred, y_qda_test)\n",
    "print('Accuracy score for Quadratic Discriminant Analysis : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"quadratic discriminant classifier\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618e2ed5",
   "metadata": {},
   "source": [
    "## 3.12 Model 12 : Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "edeb8281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for Stochastic gradient descent classifier :  0.7509565217391304\n"
     ]
    }
   ],
   "source": [
    "# Perform SGDClassifier\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_sgd = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_sgd = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_sgd_woNaN = imputer.fit_transform(X_sgd)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_sgd_woNaN = (X_sgd_woNaN[~np.isnan(y_sgd)[:,0], :])\n",
    "y_sgd_woNaN = y_sgd[~np.isnan(y_sgd)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_sgd_train, X_sgd_test, y_sgd_train, y_sgd_test = train_test_split(X_sgd_woNaN, y_sgd_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_sgd = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', SGDClassifier())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_sgd.fit(X_sgd_train, y_sgd_train)\n",
    "\n",
    "# Predicting values\n",
    "y_sgd_pred = pipe_sgd.predict(X_sgd_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_sgd_pred, y_sgd_test)\n",
    "print('Accuracy score for Stochastic gradient descent classifier : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"stochastic gradient descent\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acf8df0",
   "metadata": {},
   "source": [
    "## 3.13 Model 13 : XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1ea24cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for XGBoost classifier :  0.7617391304347826\n"
     ]
    }
   ],
   "source": [
    "# Perform XGBClassifier\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_xgb = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_xgb = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_xgb_woNaN = imputer.fit_transform(X_xgb)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_xgb_woNaN = (X_xgb_woNaN[~np.isnan(y_xgb)[:,0], :])\n",
    "y_xgb_woNaN = y_xgb[~np.isnan(y_xgb)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_xgb_train, X_xgb_test, y_xgb_train, y_xgb_test = train_test_split(X_xgb_woNaN, y_xgb_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_xgb = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42))])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_xgb.fit(X_xgb_train, y_xgb_train)\n",
    "\n",
    "# Predicting values\n",
    "y_xgb_pred = pipe_xgb.predict(X_xgb_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_xgb_pred, y_xgb_test)\n",
    "print('Accuracy score for XGBoost classifier : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"XGBoost classifier\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722e2f54",
   "metadata": {},
   "source": [
    "## 3.14 Model 14 : Gaussian process classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "96b1b5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for Gaussian process classifier :  0.735304347826087\n"
     ]
    }
   ],
   "source": [
    "# Perform GaussianProcessClassifier\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_gpc = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_gpc = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_gpc_woNaN = imputer.fit_transform(X_gpc)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_gpc_woNaN = (X_gpc_woNaN[~np.isnan(y_gpc)[:,0], :])\n",
    "y_gpc_woNaN = y_gpc[~np.isnan(y_gpc)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_gpc_train, X_gpc_test, y_gpc_train, y_gpc_test = train_test_split(X_gpc_woNaN, y_gpc_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_gpc = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', GaussianProcessClassifier())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_gpc.fit(X_gpc_train, y_gpc_train)\n",
    "\n",
    "# Predicting values\n",
    "y_gpc_pred = pipe_gpc.predict(X_gpc_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_gpc_pred, y_gpc_test)\n",
    "print('Accuracy score for Gaussian process classifier : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"gaussian process classifier\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21869b1",
   "metadata": {},
   "source": [
    "## 3.15 Model 15 : Passive aggressive classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3544f5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for Gaussian process classifier :  0.7057391304347826\n"
     ]
    }
   ],
   "source": [
    "# Perform PassiveAggressiveClassifier\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_pac = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_pac = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_pac_woNaN = imputer.fit_transform(X_pac)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_pac_woNaN = (X_pac_woNaN[~np.isnan(y_pac)[:,0], :])\n",
    "y_pac_woNaN = y_pac[~np.isnan(y_pac)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_pac_train, X_pac_test, y_pac_train, y_pac_test = train_test_split(X_pac_woNaN, y_pac_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_pac = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', PassiveAggressiveClassifier())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_pac.fit(X_pac_train, y_pac_train)\n",
    "\n",
    "# Predicting values\n",
    "y_pac_pred = pipe_pac.predict(X_pac_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_pac_pred, y_pac_test)\n",
    "print('Accuracy score for Gaussian process classifier : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"passive aggressive classifier\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f242319",
   "metadata": {},
   "source": [
    "## 3.16 Model 16 : Linear perceptron classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ce86f9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for Gaussian process classifier :  0.6803478260869565\n"
     ]
    }
   ],
   "source": [
    "# Perform Perceptron\n",
    "\n",
    "# Split dataset in features and target variable\n",
    "X_lpc = df.loc[:10000, df.columns != 'DSDECOD'].to_numpy()\n",
    "y_lpc = df.loc[:10000, df.columns == 'DSDECOD'].to_numpy()\n",
    "\n",
    "# Fill NA values\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "X_lpc_woNaN = imputer.fit_transform(X_lpc)\n",
    "\n",
    "# Get rid of rows with missing y values\n",
    "X_lpc_woNaN = (X_lpc_woNaN[~np.isnan(y_lpc)[:,0], :])\n",
    "y_lpc_woNaN = y_lpc[~np.isnan(y_lpc)]\n",
    "\n",
    "# Split X and y into training and testing sets\n",
    "X_lpc_train, X_lpc_test, y_lpc_train, y_lpc_test = train_test_split(X_lpc_woNaN, y_lpc_woNaN, test_size=0.3, random_state=16)\n",
    "\n",
    "# Create pipeline to standardize and make logistic regression\n",
    "pipe_lpc = Pipeline([('scl', StandardScaler()), \n",
    "                     ('clf', Perceptron())])\n",
    "\n",
    "# Fit data into the model\n",
    "pipe_lpc.fit(X_lpc_train, y_lpc_train)\n",
    "\n",
    "# Predicting values\n",
    "y_lpc_pred = pipe_lpc.predict(X_lpc_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "acc_score = accuracy_score(y_lpc_pred, y_lpc_test)\n",
    "print('Accuracy score for Gaussian process classifier : ',acc_score)\n",
    "df_results = df_results.append(pd.Series({\"model name\" : \"perceptron\", \"accuracy\" : acc_score}), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1bfad584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model name</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gradient boosting classifier</td>\n",
       "      <td>0.793739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>normal SVM classification</td>\n",
       "      <td>0.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>linear SVM classification</td>\n",
       "      <td>0.784348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLP classifier</td>\n",
       "      <td>0.782609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>linear discriminant analysis</td>\n",
       "      <td>0.779130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic regression</td>\n",
       "      <td>0.775652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>extra trees classifier</td>\n",
       "      <td>0.771826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>naive Bayes</td>\n",
       "      <td>0.771478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>random forest</td>\n",
       "      <td>0.770783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>k-nearest neighbors</td>\n",
       "      <td>0.763826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>XGBoost classifier</td>\n",
       "      <td>0.761739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>stochastic gradient descent</td>\n",
       "      <td>0.750957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gaussian process classifier</td>\n",
       "      <td>0.735304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>decision tree classifier</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ADABoost classifier</td>\n",
       "      <td>0.710957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>passive aggressive classifier</td>\n",
       "      <td>0.705739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>perceptron</td>\n",
       "      <td>0.680348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>quadratic discriminant classifier</td>\n",
       "      <td>0.283130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model name  accuracy\n",
       "1        gradient boosting classifier  0.793739\n",
       "5           normal SVM classification  0.792000\n",
       "6           linear SVM classification  0.784348\n",
       "7                      MLP classifier  0.782609\n",
       "11       linear discriminant analysis  0.779130\n",
       "0                 logistic regression  0.775652\n",
       "10             extra trees classifier  0.771826\n",
       "3                         naive Bayes  0.771478\n",
       "4                       random forest  0.770783\n",
       "2                 k-nearest neighbors  0.763826\n",
       "14                 XGBoost classifier  0.761739\n",
       "13        stochastic gradient descent  0.750957\n",
       "15        gaussian process classifier  0.735304\n",
       "8            decision tree classifier  0.720000\n",
       "9                 ADABoost classifier  0.710957\n",
       "16      passive aggressive classifier  0.705739\n",
       "17                         perceptron  0.680348\n",
       "12  quadratic discriminant classifier  0.283130"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by = \"accuracy\", axis = 0, ascending  = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
